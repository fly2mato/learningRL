{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def placeholder(dim=None):\n",
    "    return tf.placeholder(dtype=tf.float32, shape=combined_shape(None,dim))\n",
    "\n",
    "def placeholders(*args):\n",
    "    return [placeholder(dim) for dim in args]\n",
    "\n",
    "def placeholder_from_space(space):\n",
    "    if isinstance(space, Box):\n",
    "        return placeholder(space.shape)\n",
    "    elif isinstance(space, Discrete):\n",
    "        return tf.placeholder(dtype=tf.int32, shape=(None,))\n",
    "    raise NotImplementedError\n",
    "\n",
    "def placeholders_from_spaces(*args):\n",
    "    return [placeholder_from_space(space) for space in args]\n",
    "\n",
    "def mlp(x, hidden_layers=(32,), activation=tf.tanh, output_activation=None):\n",
    "    for h in hidden_layers[:-1]:\n",
    "        x = tf.layers.dense(x, units=h, activation=activation)\n",
    "    return tf.layers.dense(x, units=hidden_layers[-1], activation=output_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ph = placeholder_from_space(env.observation_space)\n",
    "\n",
    "action_dim = env.action_space.n\n",
    "target_ph = placeholder(action_dim)\n",
    "qvalues = mlp(x_ph, [64,action_dim])\n",
    "action = tf.argmax(qvalues, axis=1)\n",
    "qvalue_max = tf.reduce_max(qvalues, axis=1)\n",
    "\n",
    "\n",
    "qloss = tf.reduce_max(tf.squared_difference(target_ph, qvalues))\n",
    "\n",
    "qf_lr = 0.01\n",
    "train_loss = tf.train.AdamOptimizer(learning_rate=qf_lr).minimize(qloss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    x = np.array(obs)\n",
    "    target = sess.run(qvalues, feed_dict={x_ph:x[:-1]})\n",
    "    q_next = sess.run(qvalue_max, feed_dict={x_ph:x[1:]})\n",
    "    target[np.arange(target.shape[0]), act] = np.array(rew) + gamma * q_next\n",
    "    _, loss = sess.run([train_loss, qloss], feed_dict={x_ph:x[:-1], target_ph:target})\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 220.98it/s]\n"
     ]
    }
   ],
   "source": [
    "max_iter = 1000\n",
    "epsilon = 0.1\n",
    "gamma = 0.95\n",
    "episode = 1000\n",
    "episode_reward_loss = []\n",
    "\n",
    "RENDER = False\n",
    "for i in tqdm(range(episode)):\n",
    "    count = 0\n",
    "    sum_rew = 0\n",
    "    obs = []\n",
    "    act = []\n",
    "    rew = []\n",
    "\n",
    "    observation = env.reset()\n",
    "    obs.append(observation)\n",
    "    while True:\n",
    "        if RENDER: \n",
    "            #env.render()\n",
    "            pass\n",
    "                            \n",
    "        if (np.random.binomial(1, epsilon) == 0):\n",
    "            a = sess.run(action, feed_dict={x_ph:observation.reshape(1,-1)})[0]\n",
    "        else:\n",
    "            a = env.action_space.sample()\n",
    "            # = np.random.choice(np.arange(action_dim))\n",
    "        observation, reward, done, _ = env.step(a)\n",
    "        obs.append(observation)\n",
    "        act.append(a)\n",
    "        rew.append(reward)\n",
    "\n",
    "        sum_rew += reward\n",
    "        count += 1\n",
    "        if (count > max_iter or done):\n",
    "            loss = update()\n",
    "            episode_reward_loss.append([sum_rew, loss])\n",
    "            if sum_rew > 100: RENDER=True\n",
    "            #print([i,sum_rew, loss])\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
