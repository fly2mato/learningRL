# Spinning Up as a Deep RL Researcher

### 目录
- 正确的背景知识
- 边做边学
- 开展一个研究项目
- 在强化学习中进行严谨的研究工作
- Closing Thoughts
- 其他的资源
- 参考

如果您是一个有抱负的深度RL研究人员，您可能已经听说过关于深度RL的各种事情。你知道RL很难而且并不总是有效。即使你跟着一个指南做，可重复性也是一个挑战。而如果你从头开始，学习曲线将会是非常陡峭的。你也会遇到这样的情况，可以获得很多不错的资源，但是这些材料太新了，并且没有一个明确的路径指引你去掌握它们。本专栏的目标就是帮助你克服最初的障碍，并让你清晰地感受如何一步一步成为一名深度强化学习研究人员。特别地，本专栏将为你的基础知识的积累勾勒出一个有用的课程大纲，同时也将有助于你做出更好研究的零散知识整合在其中。

## 必要的背景知识

**建立坚实的数学基础。** 从概率和统计的角度，熟悉随机变量、贝叶斯定理、概率的链式法则、期望值、标准差和重要性抽样。从多元微积分角度，理解梯度和泰勒级数展开(可选，但它会有所帮助)。

**了解深度学习方面的基础知识。** 您不需要了解每一个特殊的技巧和体系结构，但是了解基础知识会对你有所帮助。了解标准体系结构(MLP、vanilla RNN、LSTM(参见本博客)、GRU、卷积层、残差网络(resnets)、注意机制), 常见的正则化方法(权重衰减、dropout)， 标准化(批处理规范、层规范、权重规范)和优化算法(随机梯度下降SGD、动量随机梯度下降momentum SGD、Adam等)。知道重参数化技巧是什么。

**熟悉至少一种深度学习架构。** Tensorflow或者PyTorch都是很好的首选。你不需要知道如何完成每件事，但应该能够很自信地写个简单程序来完成一个监督学习的任务。

**熟悉RL中的主要概念和术语。** 知道 states, actions, trajectories, policies, rewards, value functions 以及 action-value functions 的含义。如果你对这些不熟悉， 我们提供了一份对这些术语的介绍文档。当然，OpenAI Hackathon的文档、Lilian Weng异常全面的概述都是值得去看的。此外，如果你是享受数学理论的那类人，可以学习monotonic improvement theory（高级策略提升算法的构成基础）或是经典RL算法（尽管正逐步被深度RL算法取代，但其中包含的有价值的见解有时也会推动新的研究）

## 边学边做

**编写自己的实现代码。** 你应该尽可能多地自己独立实现深度强化学习的核心算法，并坚持为每种算法编写简短且正确的代码。到目前为止，这是了解那些算法如何工作以及建立对它们具体性能特征的直觉的最佳方法。

**简洁性至关重要。** 最开始你应该将全部精力放在那些最简单的算法实现上，然后再逐渐加入复杂的情况。如果你一开始就试图搭建具有太多灵活组件的东西时，它极有可能会崩溃，你也将耗费数周的时间去调试它们。这是很多深度强化学习初学者犯的通病。如果你发现自己身陷其中，不必气馁，你应该转变自己的学习方式，在回到更复杂的问题之前先去尝试研究更简单的算法。

**选择什么算法？** 你可能应该从常规的策略梯度（vanilla policy gradient, 也被称为REINFORCE）、DQN、A2C（A3C的同步版本）、PPO（带有目标裁剪的变种）以及DDPG开始，且大概按照这个顺序。上述算法的最简单版本都可以用小几百行（大约250-300）的代码实现，而且有些会更少（例如精简版的VPG只需要80行左右的代码）。在尝试实现这些算法的并行版本之前先完成单线程的代码。（尝试完成至少一种算法的并行版本）

**专注于理解。** 编写实用的强化学习代码需要对算法有清晰、详细的理解。这是因为有问题的代码总是在悄无声息间出错。在这种情况下，代码本身看起来运行良好，只是智能体永远无法学会如何去完成任务。通常这是某些量用了错误的方程进行计算，或是依赖了错误的数据分布特性，或是数据被输送到了错误的地方。很多时候解决这些问题的唯一办法只能是用苛刻的眼光仔细阅读源码，确切地弄明白它应该做什么，并找到它在什么地方偏离了正常的行为。这些知识的具备需要你去接触学术文献以及其他的已有实现方法。所以你应该将大量的时间花费在阅读这些内容上。

**在论文中应关注的地方：** 当复现一篇论文里的算法时，应该来回反复的阅读，尤其是ablation analyses和补充材料（如果有的话）。ablation有助于你建立诊断错误的直觉，类似于什么参数或是子函数对算法起作用的影响最大。补充材料往往会提供关于具体细节的信息，比如网络结构和优化的超参数。你应该尝试着将你的实现与这些细节联系起来，从而提高代码起作用的机率。

**但不要过于注重论文的细节。** 有时论文规定使用了一些超过必要的技巧，所以请注意这一点，并在可能的情况下尝试进行简化。举例来说，最初的DDPG论文提出了一种复杂的神经网络结构和初始化方法，以及批处理标准化。然而这些并不是严格必需的，有一些使用更简单网络的DDPG同样获得了非常好的结果;另一个例子则是原始A3C论文使用了源自各种actor-learner的异步更新，但研究结果表明同步更新同样表现很好。

**同样不要过于关注已有的实现。** 可以从已有的实现中汲取灵感，但是注意不要过于关注这些实现代码中的工程细节。强化学习代码库通常为了不同算法间的代码复用而选择抽象化的实现，但对于仅仅实现单一算法或是实现单一功能的你并不是必需的。

**在简单环境中快速迭代。** 当需要调试你的代码时，尝试使用简单的环境，类似OpenAI Gym 提供的CartPole-v0, InvertedPendulum-v0, FrozenLake-v0 以及 HalfCheetah-v2 (仅仅100～250步的短时间窗口，而不是完整的1000步) 这种学习频率很高的环境。如果你没有事先在这些最简单的任务上进行验证，请不要尝试将算法抛在Atari或是复杂的类人环境中。在调试阶段，理想的实验周转时间是小于5分钟(在本地机器上)或稍长一些。这些小规模的实验不需要任何特殊的硬件，并且在cpu上运行时也不会遇到太多麻烦。

**当代码不工作时，假设写了一个BUG。** 在你求助于调整超参数之前，请先花费一定的精力查找一下BUG：通常问题的原因都源自一个BUG。差的超参数能够显著衰减强化学习算法的性能，但是如果你使用的是与论文中或是标准库中相似的超参数，那就很有可能不是超参数的问题。另一个值得记住的点：有时候即使代码中存在严重的BUG也有可能在某个环境中正常工作。所以当你的结果看起来不错的时候，请确保在多个环境中进行测试。

**衡量一切。** 多准备一些手段去观察代码运行过程中发生了什么。在学习过程的每一次迭代中，你观察到的状态越多越容易去调试代码。如果你没办法观察到它失败了，你就不能确定它是否失败了。我个人喜欢去观察累积奖励均值/标准差/最小值/最大值、一次训练的长度、值函数估计、目标的损失函数以及任何探索性参数的细节（类似随机策略优化中的平均熵、DQN中epsilon-greedy算法的当前epsilon值）。当然，不时地观察一下智能体表现的视频记录，这会给你一些在其他情况下得不到的洞见。

**代码工作时进行规模化实验。** 当你有了一个似乎可以在最简单的环境中正确工作的RL算法实现之后，可以在更复杂的环境中进行试验。在这个阶段的实验需要更长的时间，视情况从几个小时到几天不等。这个阶段中，类似强性能GPU或是32核主机这种专业的硬件设备将会提供很大的帮助。同时你也应该考虑寻求云计算，类似AWS或是GCE。

**保持上述习惯！** 当你跨过深度强化学习的初始阶段之后，上述的这些习惯仍然是值得保持，因为它们能有助于加速你的研究工作！

## 开展一个研究项目